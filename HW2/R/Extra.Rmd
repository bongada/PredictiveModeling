---
title: "Extra"
output: html_document
---
To apply Naive Bayes theorem, we need to split up the matrix so that each author has their own matrix of only articles that they wrote. So rows 1-49 should be assigned to Aaron Pressman's training set, and so on, until all 2500 rows/articles are assigned to their correct author. We'll number the authors 1 - 50 so that looping through is easier.
```{r}
newX <- split(as.data.frame(X),rep(1:50, each = 50))
summary(newX)
```
This returned a list of matricies which is called newX. To refer to one author, we would do the following:
```{r, echo = FALSE}
newX[0:1]
```

```{r}
# smooth_count = 1/nrow(X)
# NaiveBayes = function(matrix){
#   w_x = colSums(matrix + smooth_count)
#   w_x = w_x/sum(w_x)
#   return w_x
# }
# NaiveBayes(X[0:50,])
# w_x = colSums(newX[0:1] + smooth_count)
# smooth_count
# 
# 
# for (row in 1:2500){
#   A <- lapply (1:50, NaiveBayes(X[row:row+50,]))
#   row = row+50
# }


This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
summary(cars)
```

You can also embed plots, for example:

```{r, echo=FALSE}
plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


```{r}
for (row in seq(from=0, to=2450, by=50)){
  X_test2[row:row+50,] = X_test[row:row+50, -which(!(names(X_test) %in% names(X)))]
}
# X_train2 = X
# for (row in seq(from=0, to=2450, by=50)){
#   X_train2[row:row+50,] = X[row:row+50, -which(!(names(X) %in% names(X_test)))]
# }
```


```{r, echo = FALSE}
smooth_count = 1/nrow(DTM_training)
w_AP = colSums(DTM_training[0:50,] + smooth_count)
w_AP = w_AP/sum(w_AP)
w_AC = colSums(DTM_training[50:100,] + smooth_count)
w_AC = w_AC/sum(w_AC)
w_AS = colSums(DTM_training[100:150,] + smooth_count)
w_AS = w_AS/sum(w_AS)
w_BKL = colSums(DTM_training[150:200,] + smooth_count)
w_BKL = w_BKL/sum(w_BKL)
w_BH = colSums(DTM_training[200:250,] + smooth_count)
w_BH = w_BH/sum(w_BH)
w_BD = colSums(DTM_training[250:300,] + smooth_count)
w_BD = w_BD/sum(w_BD)
w_DS = colSums(DTM_training[300:350,] + smooth_count)
w_DS = w_DS/sum(w_DS)
w_DL = colSums(DTM_training[350:400,] + smooth_count)
w_DL = w_DL/sum(w_DL)
w_EF = colSums(DTM_training[400:450,] + smooth_count)
w_EF = w_EF/sum(w_EF)
w_EA = colSums(DTM_training[450:500,] + smooth_count)
w_EA = w_EA/sum(w_EA)
w_FF = colSums(DTM_training[500:550,] + smooth_count)
w_FF = w_FF/sum(w_FF)
w_GE = colSums(DTM_training[550:600,] + smooth_count)
w_GE = w_GE/sum(w_GE)
w_HS = colSums(DTM_training[600:650,] + smooth_count)
w_HS = w_HS/sum(w_HS)
w_JMA = colSums(DTM_training[650:700,] + smooth_count)
w_JMA = w_JMA/sum(w_JMA)
w_JL = colSums(DTM_training[700:750,] + smooth_count)
w_JL = w_JL/sum(w_JL)
w_JG = colSums(DTM_training[750:800,] + smooth_count)
w_JG = w_JG/sum(w_JG)
w_JO = colSums(DTM_training[800:850,] + smooth_count)
w_JO = w_JO/sum(w_JO)
w_JM = colSums(DTM_training[850:900,] + smooth_count)
w_JM = w_JM/sum(w_JM)
w_JB = colSums(DTM_training[900:950,] + smooth_count)
w_JB = w_JB/sum(w_JB)
w_JW = colSums(DTM_training[950:1000,] + smooth_count)
w_JW = w_JW/sum(w_JW)
w_KP = colSums(DTM_training[1000:1050,] + smooth_count)
w_KP = w_KP/sum(w_KP)
w_KW = colSums(DTM_training[1050:1100,] + smooth_count)
w_KW = w_KW/sum(w_KW)
w_KD = colSums(DTM_training[1100:1150,] + smooth_count)
w_KD = w_KD/sum(w_KD)
w_KM = colSums(DTM_training[1150:1200,] + smooth_count)
w_KM = w_KM/sum(w_KM)
w_KR = colSums(DTM_training[1200:1250,] + smooth_count)
w_KR = w_KR/sum(w_KR)
w_KK = colSums(DTM_training[1250:1300,] + smooth_count)
w_KK = w_KK/sum(w_KK)
w_LZ = colSums(DTM_training[1300:1350,] + smooth_count)
w_LZ = w_LZ/sum(w_LZ)
w_LO = colSums(DTM_training[1350:1400,] + smooth_count)
w_LO = w_LO/sum(w_LO)
w_LB = colSums(DTM_training[1400:1450,] + smooth_count)
w_LB = w_LB/sum(w_LB)
w_MM = colSums(DTM_training[1450:1500,] + smooth_count)
w_MM = w_MM/sum(w_MM)
w_MB = colSums(DTM_training[1500:1550,] + smooth_count)
w_MB = w_MB/sum(w_MB)
w_MW = colSums(DTM_training[1550:1600,] + smooth_count)
w_MW = w_MW/sum(w_MW)
w_MBU = colSums(DTM_training[1600:1650,] + smooth_count)
w_MBU = w_MBU/sum(w_MBU)
w_MC = colSums(DTM_training[1650:1700,] + smooth_count)
w_MC = w_MC/sum(w_MC)
w_MD = colSums(DTM_training[1700:1750,] + smooth_count)
w_MD = w_MD/sum(w_MD)
w_NL = colSums(DTM_training[1750:1800,] + smooth_count)
w_NL = w_NL/sum(w_NL)
w_PC = colSums(DTM_training[1800:1850,] + smooth_count)
w_PC = w_PC/sum(w_PC)
w_PH = colSums(DTM_training[1850:1900,] + smooth_count)
w_PH = w_PH/sum(w_PH)
w_PT = colSums(DTM_training[1900:1950,] + smooth_count)
w_PT = w_PT/sum(w_PT)
w_RS = colSums(DTM_training[1950:2000,] + smooth_count)
w_RS = w_RS/sum(w_RS)
w_RF = colSums(DTM_training[2000:2050,] + smooth_count)
w_RF = w_RF/sum(w_RF)
w_SP = colSums(DTM_training[2050:2100,] + smooth_count)
w_SP = w_SP/sum(w_SP)
w_SD = colSums(DTM_training[2100:2150,] + smooth_count)
w_SD = w_SD/sum(w_SD)
w_SH = colSums(DTM_training[2150:2200,] + smooth_count)
w_SH = w_SH/sum(w_SH)
w_SC = colSums(DTM_training[2200:2250,] + smooth_count)
w_SC = w_SC/sum(w_SC)
w_TEL = colSums(DTM_training[2250:2300,] + smooth_count)
w_TEL = w_TEL/sum(w_TEL)
w_TP = colSums(DTM_training[2300:2350,] + smooth_count)
w_TP = w_TP/sum(w_TP)
w_TF = colSums(DTM_training[2350:2400,] + smooth_count)
w_TF = w_TF/sum(w_TF)
w_TN = colSums(DTM_training[2400:2450,] + smooth_count)
w_TN = w_TN/sum(w_TN)
w_WK = colSums(DTM_training[2450:2500,] + smooth_count)
w_WK = w_WK/sum(w_WK)
```

Now to test a few documents from Aaron Pressman's test folder:
```{r}
x_test = DTM_testing[4,]
head(sort(x_test, decreasing=TRUE), 25)
test_AP = sum(x_test*log(w_AP))
test_AC = sum(x_test*log(w_AC))
test_AS = sum(x_test*log(w_AS))
test_BKL = sum(x_test*log(w_BKL))
test_BH = sum(x_test*log(w_BH))
test_BD = sum(x_test*log(w_BD))
test_DS = sum(x_test*log(w_DS))
test_DL = sum(x_test*log(w_DL))
test_EF = sum(x_test*log(w_EF))
test_EA = sum(x_test*log(w_EA))
test_FF = sum(x_test*log(w_FF))
test_GE = sum(x_test*log(w_GE))
test_HS = sum(x_test*log(w_HS))
test_JMA = sum(x_test*log(w_JMA))
test_JL = sum(x_test*log(w_JL))
test_JG = sum(x_test*log(w_JG))
test_JO = sum(x_test*log(w_JO))
test_JM = sum(x_test*log(w_JM))
test_JB = sum(x_test*log(w_JB))
test_JW = sum(x_test*log(w_JW))
test_KP = sum(x_test*log(w_KP))
test_KW = sum(x_test*log(w_KW))
test_KD = sum(x_test*log(w_KD))
test_KM = sum(x_test*log(w_KM))
test_KR = sum(x_test*log(w_KR))
test_KK = sum(x_test*log(w_KK))
test_LZ = sum(x_test*log(w_LZ))
test_LO = sum(x_test*log(w_LO))
test_LB = sum(x_test*log(w_LB))
test_MM = sum(x_test*log(w_MM))
test_MB = sum(x_test*log(w_MB))
test_MW = sum(x_test*log(w_MW))
test_MBU = sum(x_test*log(w_MBU))
test_MC = sum(x_test*log(w_MC))
test_MD = sum(x_test*log(w_MD))
test_NL = sum(x_test*log(w_NL))
test_PC = sum(x_test*log(w_PC))
test_PH = sum(x_test*log(w_PH))
test_PT = sum(x_test*log(w_PT))
test_RS = sum(x_test*log(w_RS))
test_RF = sum(x_test*log(w_RF))
test_SP = sum(x_test*log(w_SP))
test_SD = sum(x_test*log(w_SD))
test_SH = sum(x_test*log(w_SH))
test_SC = sum(x_test*log(w_SC))
test_TEL = sum(x_test*log(w_TEL))
test_TP = sum(x_test*log(w_TP))
test_TF = sum(x_test*log(w_TF))
test_TN = sum(x_test*log(w_TN))
test_WK = sum(x_test*log(w_WK))
max(test_AP, test_AC, test_AS, test_BKL, test_BH, test_BD, test_DS, test_DL, test_EF, test_EA, test_FF, test_GE, test_HS, test_JM, test_JL, test_JG, test_JG, test_JO, test_JMA, test_JB, test_JW, test_KP, test_KW, test_KD, test_KM, test_KR, test_KK, test_LZ, test_LO, test_LB, test_MM, test_MB, test_MW, test_MBU, test_MC, test_MD, test_NL, test_PC, test_PH, test_PT, test_RS, test_RF, test_SP, test_SD, test_SH, test_SC, test_TEL, test_TP, test_TF, test_TN, test_WK)
w_WK
```

PCA:
```{r}
X = as.matrix(DTM_simon)
X = X/rowSums(X)  # term-frequency weighting

pca_simon = prcomp(X, scale=TRUE)
plot(pca_simon) 

# Look at the loadings
pca_simon$rotation[order(abs(pca_simon$rotation[,1]),decreasing=TRUE),1][1:25]
pca_simon$rotation[order(abs(pca_simon$rotation[,2]),decreasing=TRUE),2][1:25]


## Plot the first two PCs..
plot(pca_simon$x[,1:2], col='grey', pch=19, xlab="PCA 1 direction", ylab="PCA 2 direction", bty="n")
identify(pca_simon$x[,1:2], n=4)
```


```{r}
library(ggplot2)
plot = ggplot(NB_Pred_Table)
plot + geom_tile(aes(x=testing_labels, y=NB_Pred_Table, fill=Freq)) + 
  scale_x_discrete(name="Actual Class") + 
  scale_y_discrete(name="Predicted Class") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```

Create a bar graph that has the number of flights in for each carrier in one color, and laid on top, the number of flights out.
Steps:
- read in the data
- get a subset of this data that contains all of the flights with the unique carrier, the origin and whether or not the flight was cancelled
- get a subset of this subset that only contains flights that were cancelled
- ausOrNot is a boolean that indicates whether a flight originated in Austin or Not
- graph the unique carrier versus the freq and color ausOrNot
- for each airliner, and for the flights that were cancelled, find out how many were incoming and how many were outgoing
- need a loop for each of the airliners: 9E, AA, B6, CO, DL, EV, F9, MQ, NW, OH, OO, UA, US, WN, XE, YV
```{r}
#want the sum of the flights that were cancelled for each unique carrier where orgin = 'AUS' and origin != 'AUS'

x = sqldf("select count(cancelled) from subflights where origin = 'AUS', uniquecarrier = '9E', cancelled = 1 group by uniquecarrier")

#SELECT sum(cancelled)
#FROM subflights
#GROUP BY uniquecarrier
#WHEN origin = 'AUS'

head(subflights)
for (i in 1:nrow(subflights)){
  if (subflights$UniqueCarrier == '9E'){
    
  }
}

